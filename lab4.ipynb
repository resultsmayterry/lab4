{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 4: Introduction to Reinforcement Leaerning\n",
    "\n",
    "In this lab we will get a small taste of the power Reinforcement Learning (RL) has to offer when it comes to robotics and control. Your task will be to use the [gym](https://gym.openai.com/docs/) library to simulate a small MDP environment, and use tabular Q-learning to solve it. \n",
    "\n",
    "Given the interdisciplinary nature of RL, the breadth of the topic can easily fill multiple courses. Here, you will be introduced to the math you need to succesfully implement Q-learning, but the reasoning behind all the steps is not explained. I recommend starting at this [lecture](https://www.youtube.com/watch?v=E3f2Camj0Is&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u&index=2) for those interested. Note that \n",
    "it takes three lectures (2-4) to arrive at the Q-learning formulation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Overview\n",
    "### I. gym\n",
    "### II. Markov Decision Processes\n",
    "### III. Q-learning\n",
    "\n",
    "### You are responsible for completing the tasks explained in:\n",
    "- **Cells 7-8**: Your tabular Q-learning implementation.\n",
    "- **Cells 11-12**: A plot of your training statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. gym\n",
    "\n",
    "The [gym](https://gym.openai.com/docs/) library is a benchmark suite for RL provided by OpenAI. It is widely used in research due to its easy integration with python.\n",
    "\n",
    "You can install it below as we did for scipy in the last lab, or use the new yml file provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np \n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a ton of environments to choose from with gym, they can all be accessed using their associated name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\").env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've made the environemnt, you can directly interact with the env object created.\n",
    "\n",
    "Here we will look at the taxi environment, where the goal of the agent/robot/taxi is to pick up a passenger from one of  four possible locations, and drop them off at another. Since the environment is small and does not relocate the obstacles it is easy to solve methodically, but we will see how a general algorithm can be used to find the ideal policy.  \n",
    "\n",
    "We can take a look at what's going on up close:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Taxi-v3](https://gym.openai.com/envs/Taxi-v3/)\n",
    "\n",
    "A thorough explanation directly from the [source](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py):\n",
    "\n",
    "#\n",
    "    The Taxi Problem\n",
    "    from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\"\n",
    "    by Tom Dietterich\n",
    "\n",
    "    Description:\n",
    "    There are four designated locations in the grid world indicated \n",
    "    by R(ed), G(reen), Y(ellow), and B(lue). \n",
    "    When the episode starts, the taxi starts off at a random square \n",
    "    and the passenger is at a random location. \n",
    "    The taxi drives to the passenger's location, \n",
    "    picks up the passenger, drives to the passenger's destination \n",
    "    (another one of the four specified locations), and then drops off the passenger. \n",
    "    Once the passenger is dropped off, the episode ends.\n",
    "    Observations:\n",
    "    There are 500 discrete states since there are 25 taxi positions, \n",
    "    5 possible locations of the passenger, and 4 destination locations. \n",
    "\n",
    "    Passenger locations:\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "    - 4: in taxi\n",
    "\n",
    "    Destinations:\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "\n",
    "    Actions:\n",
    "    There are 6 discrete deterministic actions:\n",
    "    - 0: move south\n",
    "    - 1: move north\n",
    "    - 2: move east\n",
    "    - 3: move west\n",
    "    - 4: pickup passenger\n",
    "    - 5: drop off passenger\n",
    "\n",
    "    Rewards:\n",
    "    There is a default per-step reward of -1,\n",
    "    except for delivering the passenger, which is +20,\n",
    "    or executing \"pickup\" and \"drop-off\" actions illegally, which is -10.\n",
    "    \n",
    "    Rendering:\n",
    "    - blue: passenger\n",
    "    - magenta: destination\n",
    "    - yellow: empty taxi\n",
    "    - green: full taxi\n",
    "    - other letters (R, G, Y and B): locations for passengers and destinations\n",
    "    \n",
    "    state space is represented by:\n",
    "        (taxi_row, taxi_col, passenger_location, destination)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we interact with the environment?\n",
    "\n",
    "Essentially you only need two methods: a way to reset the environment once the passenger has been dropped off, and a way to take a specific action. \n",
    "\n",
    "We first take a look at the env.reset() method. When you reset a gym environment, it returns the initial state of the env. Technically the state should be a 4-dim vector: (taxi_row, taxi_col, passenger_location, destination), but since there are only 500 possible states the vector is encoded to an integer value. If you want to make sense of this value in its original representation you can use the provided env.decode() method.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 1, 2]\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = env.reset()\n",
    "print(list(env.decode(output)))\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay that was the reset, now the second ingredient we need is a way to control the env. We can do this with env.step(), where the input should be an integer b/w 0 and 5 as described above. Once we take our desired action with env.step(), it returns 4 items: the next state,the reward recieved for taking our action, a boolean that checks if we are done, and information regarding the states visited (we dont need this last part). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : :\u001b[43m \u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "state: \t [1, 4, 0, 1]\n",
      "rew: \t -10\n",
      "done: \t False\n"
     ]
    }
   ],
   "source": [
    "state, rew, done, _ = env.step(4)\n",
    "env.render()\n",
    "print('state: \\t', list(env.decode(state)))\n",
    "print('rew: \\t', rew)\n",
    "print('done: \\t', done)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Markov Decision Processes\n",
    "Markov Chains (MC) are powerful tools for modeling the dynamics of systems. What if we wanted to use an MC to model a certain game, evaluating different policies to compare them? We can just use MCs, but since we want a good metric to evaluate these policies, we need to consider taking different actions and recieving a reward signal for feedback. At some state $s_t$ in the MC, we can take an action $a_t$, and observe not only the next state $s_{t+1}$, but also the reward $r_t$ associated with that transition. This is how Markov Decision Processes (MDP) are formed i.e., they combine elements of control with the Markovian model.  \n",
    "\n",
    "MDPs rely on the Markov Assumption:\n",
    "$$\n",
    "P(s_{t+1}|s_t,a_t)=P(s_{t+1}|h_t,a_t),\n",
    "$$\n",
    "where $h_t$ is the history of all states up to step $t$. Essentially, this says that we only need to know the current state and the action to predict the outcome. With this assumption, we can formulate an MDP $\\mathcal{M}:\\{\\mathcal{S},\\mathcal{A},P,R\\}$ as a collection of a set of states, a set of actions, a probability transition function and a reward transition function respectively. \n",
    "\n",
    "We also need a way to express the policy:  \n",
    "$$\\pi: \\mathcal{S}\\mapsto\\mathcal{A},$$\n",
    "where in the determenistic case $\\pi(s)=a$. Otherwise $\\pi(a|s) = Pr.(a_t=a|s_t=s)$\n",
    "\n",
    "At every state $s_t$, we receive a reward $r_t=R(s,a)$ for taking action $a_t$. By rolling out a certain policy $\\pi$ in our MDP, we can collect what we call the return at step $t$:\n",
    "$$\n",
    "G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\ldots.\n",
    "$$\n",
    "You can think of this as the total payout we will get if we run polciy $\\pi$ in our given MDP, where $\\gamma$ is a the discount factor we give towards rewards we receive in the future. We set $\\gamma<1$ in order to give higher weight to rewards that come sooner, but also this makes sure that our return $G_t$ is finite even if our policy runs for an infinite time. This way we do not have to worry about the horizon of our MDP. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value and Q Function.\n",
    "A major component in reinforcement learning is keeping track of the expected return a certain policy will provide in an environment. This is defined as the value function:\n",
    "$$\n",
    "V^{\\pi}(s_t=s)=E_{\\pi}[G_t|s_t=s].\n",
    "$$\n",
    "If we knew the value function for any policy $\\pi$, we can easily compare the quality of different polcieis. But we can go one step further with our evaluation. Say at a given state $s_t$ we wanted to evaluate our policy $\\pi$, but only after taking a certain action $a_t$, and then continuing with our policy $\\pi$. This is defined as the Q-function: \n",
    "$$\n",
    "Q^{\\pi}(s_t=s,a_t=a) = R(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}P(s'|s,a)V^{\\pi}(s')\n",
    "$$\n",
    "The methodology here is that we can now evaluate our off policy decision to take action $a$. Since we know how we can estimate the value function, computing the Q-values allows us to take $\\max_a Q^\\pi(s,a)$ instead of following the policy, since this is gauranteed to be atleast as good as following $\\pi$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Tabular Q-Learning\n",
    "In Q-Learning we measure the Temporal Difference between **target** and **current estimate** of the Q-function. \n",
    "$$\n",
    "\\begin{align}\n",
    "\\textrm{target: }&\\qquad r_t + \\gamma\\max_{a'}Q(s_{t+1},a')\\\\\n",
    "\\textrm{estimate: }&\\qquad Q(s_t,a_t)\n",
    "\\end{align}\n",
    "$$\n",
    "Their difference measures the error between what the environment is telling us, and our current estimate. We can use this error to update our current estimate, which formulates the Q-learning algorithm:\n",
    "$$\n",
    "Q(s_t,a_t) := Q(s_t,a_t)+\\alpha\\Big(r_t+\\gamma\\max_{a'}Q(s_{t+1},a')-Q(s_t,a_t)\\Big).\n",
    "$$\n",
    "Now we just need a way to evaluate the policy, for which we use the epsilon-greedy approach: \n",
    "$$\n",
    "\\pi(a|s)=\n",
    "\\begin{cases}\n",
    "\\textrm{arg}\\max_a Q^\\pi(s,a)\\qquad \\textrm{w. prob } 1-\\epsilon,\\\\\n",
    "\\textrm{take a random action}\\qquad \\textrm{w. prob } \\epsilon.\n",
    "\\end{cases}\n",
    "$$\n",
    "Note how $\\epsilon$ controls how much we $\\textit{explore}$ the environment versus how much we $\\textit{exploit}$ our policy. A lot of RL is posed as a battle between exploration and exploitation, hence $\\epsilon$ is an important parameter. Typically we want to decrease the amount we explore as our policy gets better, and we do this by adapting some sort of **decay** to $\\epsilon$. In addition, since $\\alpha$ controls the speed at which we update our policy, you can consider adapting some decay as well. \n",
    "\n",
    "I will leave this element for you to experiment with, as it serves as an interesting challenge to try and find the correct parameters and fine tune. As a reference, you should be able to find the policy within 20,000 episodes (where episodes end when the taxi drops off the passenger.) In fact, even 5,000 is enough but you will see slight improvement following. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your task.\n",
    "\n",
    "The actual algorithm can be written within one cell, or broken up into smaller components, it is your choice. There are only two things I require:\n",
    "\n",
    "(1) **Represent your Q-table as an np.array of size (500,6)**, hence 500 states and 6 actions. This will let you use my policy evaluation function that follows to check if you found the solution. You might want to take a look at that function first if you have any confusion about how to interact with the gym env. \n",
    "\n",
    "(2) **Collect reward statistic of your policy over the 20,000 episodes, and plot it yourself.** You will  probably want this statistic for yourself anyway, but the plotting component is added so that we can better visualize our data. Typically we consider the episodic reward collected over one rollout, but note that using this directly will give youa noisy result. I leave the figure making up to you this time, feel free to use the templates or anything else that you like! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task (1):\n",
    "\n",
    "Your Q-learning algorithm goes below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning parameters\n",
    "eps = .1\n",
    "lr = .75\n",
    "df = 1\n",
    "\n",
    "# define optimal action lookup function\n",
    "optimal = lambda state: np.argmax(Q_table[state])\n",
    "\n",
    "# initialize rng\n",
    "rng = default_rng()\n",
    "\n",
    "# init state action table\n",
    "Q_table = np.zeros((500,6))\n",
    "\n",
    "# init episode reward vector\n",
    "ep_rewards = np.zeros(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train\n",
    "for i in range(20000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    accum_reward = 0\n",
    "    while not done:\n",
    "# choose action\n",
    "        if rng.uniform() > eps:\n",
    "                action = optimal(state)\n",
    "        else:\n",
    "                action = rng.integers(6)\n",
    "# perform action, updating state, reward, stop condition\n",
    "        newstate, reward, done, _ = env.step(action)\n",
    "        accum_reward += reward\n",
    "# compute temporal difference\n",
    "        td = reward + df*Q_table[newstate, optimal(newstate)] - Q_table[state, action]\n",
    "# update Q value for previous state\n",
    "        Q_table[state, action] += lr*td\n",
    "# prepare for next step\n",
    "        state = newstate\n",
    "    ep_rewards[i] = accum_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(env, Q_table):\n",
    "    '''\n",
    "    Evaluates the environment using the given policy \n",
    "    Inputs:\n",
    "        env - gym env object\n",
    "        Q   - policy, we will use arg max Q. \n",
    "\n",
    "    Note that I use count to make sure we exit the \n",
    "    rendering if our policy takes longer than 50 steps. \n",
    "    This way we can observe arbitrary Q tables, feel\n",
    "    free to tune this value.  \n",
    "    '''\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    done = False\n",
    "    count = 50\n",
    "    while not done:\n",
    "        count-=1\n",
    "        if count <= 0:\n",
    "            return\n",
    "\n",
    "        act = np.argmax(Q_table[state])\n",
    "        state, rew, done, _ = env.step(act)\n",
    "        env.render()\n",
    "\n",
    "        print('state: \\t', list(env.decode(state)))\n",
    "        print('rew: \\t', rew)\n",
    "        print('done: \\t', done)\n",
    "    # return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate our policy using the Q table. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[43mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | :\u001b[43m \u001b[0m:G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "state: \t [0, 3, 3, 2]\n",
      "rew: \t -1\n",
      "done: \t False\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "state: \t [1, 3, 3, 2]\n",
      "rew: \t -1\n",
      "done: \t False\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "state: \t [2, 3, 3, 2]\n",
      "rew: \t -1\n",
      "done: \t False\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "state: \t [3, 3, 3, 2]\n",
      "rew: \t -1\n",
      "done: \t False\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "state: \t [4, 3, 3, 2]\n",
      "rew: \t -1\n",
      "done: \t False\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[42mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "state: \t [4, 3, 4, 2]\n",
      "rew: \t -1\n",
      "done: \t False\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "state: \t [3, 3, 4, 2]\n",
      "rew: \t -1\n",
      "done: \t False\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "state: \t [2, 3, 4, 2]\n",
      "rew: \t -1\n",
      "done: \t False\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "state: \t [2, 2, 4, 2]\n",
      "rew: \t -1\n",
      "done: \t False\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "state: \t [2, 1, 4, 2]\n",
      "rew: \t -1\n",
      "done: \t False\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "state: \t [2, 0, 4, 2]\n",
      "rew: \t -1\n",
      "done: \t False\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "state: \t [3, 0, 4, 2]\n",
      "rew: \t -1\n",
      "done: \t False\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "state: \t [4, 0, 4, 2]\n",
      "rew: \t -1\n",
      "done: \t False\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "state: \t [4, 0, 2, 2]\n",
      "rew: \t 20\n",
      "done: \t True\n"
     ]
    }
   ],
   "source": [
    "eval_policy(env, Q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to use this template for your second task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import matplotlib as mpl\n",
    "# colors used\n",
    "ORANGE = '#FF9132'\n",
    "TEAL = '#0598B0'\n",
    "GREEN = '#008F00'\n",
    "PURPLE = '#8A2BE2'\n",
    "GRAY = '#969696'\n",
    "FIG_WIDTH = 4\n",
    "FIG_HEIGHT = 3\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": False,\n",
    "    \"font.family\": \"DejaVu Sans\",\n",
    "    \"font.serif\": [\"Palatino\"],\n",
    "    \"font.size\": 12,\n",
    "    \"axes.titlesize\": 15,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"lines.linewidth\": 2\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task (2):\n",
    "\n",
    "Plot your data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21573ae4ac0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ep_rewards)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e08d1f9f2fd15dc5471a6ea7be4d5e4074f6c78585af0bb1e5038cee0be00f0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('lab4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "e14277392997ec7b8d0d4e1eacc6a12462d831e78f154a1c368aae24d965e9e8"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
